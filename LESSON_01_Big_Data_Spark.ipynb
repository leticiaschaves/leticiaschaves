{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leticiaschaves/leticiaschaves/blob/main/LESSON_01_Big_Data_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando Spark - para python"
      ],
      "metadata": {
        "id": "V_WYmRpkLEb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi0TqVOrEdbe",
        "outputId": "4068c86f-075c-4914-856d-1fece149e896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importa a biblioteca Pyspark"
      ],
      "metadata": {
        "id": "fbjgi27fLSL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "o3asXpO5E5lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conecta com banco SQL. Como uma fila de processamento."
      ],
      "metadata": {
        "id": "l3sboetMLh3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"teste\").getOrCreate()"
      ],
      "metadata": {
        "id": "2L_0oVQMFNo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Armazena as funções da biblioteca Pyspark como F"
      ],
      "metadata": {
        "id": "zhYhRjJrLpjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "bNfJneI9FnR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark lê o arquivo CSV"
      ],
      "metadata": {
        "id": "KFIlrzSeL3VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sal = spark.read.csv(\"sal_data.csv\", header=True)"
      ],
      "metadata": {
        "id": "sh9yHkbIGI5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função **explain()**:\n",
        "Prints the (logical and physical) plans to the console for debugging purpose.\n",
        "\n",
        "### Parameters\n",
        "extendedbool, optional\n",
        "default False.\n",
        "**If False**, prints only the physical plan. When this is a string without specifying the mode, it works as the mode is specified.\n",
        "\n",
        "modestr, optional\n",
        "specifies the expected output format of plans.\n",
        "*   **simple:** Print only a physical plan.\n",
        "*   **extended:** Print both logical and physical plans.\n",
        "*   **codegen:** Print a physical plan and generated codes if they are available.\n",
        "*   **cost:** Print a logical plan and statistics if they are available.\n",
        "*   **formatted: **Split explain output into two sections: a physical plan outline and node details.\n",
        "\n",
        "Changed in version 3.0.0: Added optional argument mode to specify the expected output format of plans."
      ],
      "metadata": {
        "id": "GyIh8Oc3MNbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sal.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZUoy9379eoj",
        "outputId": "01a60108-9450-48b4-a516-95f6aae8251f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "FileScan csv [Employee_ID#17,CPF#18,First_Name#19,Last_Name#20,Basic_Salary#21] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/sal_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Employee_ID:string,CPF:string,First_Name:string,Last_Name:string,Basic_Salary:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função **show()**:\n",
        "Prints the first n rows to the console.\n",
        "\n",
        "### Parameters\n",
        "**nint, optional**\n",
        "\n",
        "Number of rows to show.\n",
        "\n",
        "**truncatebool, optional**\n",
        "\n",
        "If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length truncate and align cells right.\n",
        "\n",
        "**verticalbool, optional**\n",
        "\n",
        "If set to True, print output rows vertically (one line per column value)."
      ],
      "metadata": {
        "id": "R_UWXBkBNryn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sal.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkHgLNa4H111",
        "outputId": "88321864-bb14-46f6-f795-1ed4edbe6ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+----------+---------+------------+\n",
            "|Employee_ID|           CPF|First_Name|Last_Name|Basic_Salary|\n",
            "+-----------+--------------+----------+---------+------------+\n",
            "|     E-1001|557.216.676-64|    Mahesh|    Joshi|       16860|\n",
            "|     E-1002|647.770.644-09|    Rajesh|    Kolte|       14960|\n",
            "|     E-1004|785.276.601-30|     Priya|     Jain|       12670|\n",
            "|     E-1005|127.891.042-55|     Sneha|    Joshi|       15660|\n",
            "|     E-1007|635.807.316-88|       Ram|   Kanade|       15850|\n",
            "+-----------+--------------+----------+---------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função **withColumn()**:\n",
        "Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
        "\n",
        "The column expression must be an expression over this DataFrame; attempting to add a column from some other DataFrame will raise an error.\n",
        "\n",
        "New in version 1.3.0.\n",
        "\n",
        "### Parameters\n",
        "colNamestr\n",
        "string, name of the new column.\n",
        "\n",
        "colColumn\n",
        "a Column expression for the new column.\n",
        "\n",
        "## Função **lit()**:\n",
        "Creates a Column of literal value."
      ],
      "metadata": {
        "id": "sx60lHAYOSje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dobrado = df_sal.withColumn(\"salario_dobrado\", F.col(\"Basic_Salary\") * F.lit(2))"
      ],
      "metadata": {
        "id": "0kTfUv_2ILid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atribui a 'nomes' (nova variável) a variável criada acima (dobrado)"
      ],
      "metadata": {
        "id": "bEoJleMgO7Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nomes = dobrado"
      ],
      "metadata": {
        "id": "v3SySyLqKWad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faz um loop in para percorrer os nomes das colunas e deixar todas em minúsculo.\n",
        "\n",
        "## Função **lower()**:\n",
        "Converts a string expression to lower case.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uD2X1cK0POan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for coluna in nomes.columns:\n",
        "  nomes = nomes.withColumnRenamed(coluna, coluna.lower())"
      ],
      "metadata": {
        "id": "IdCjUM0MLk-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função **concat()**:\n",
        "Concatenates multiple input columns together into a single column. The function works with strings, binary and compatible array columns.\n",
        "\n",
        "## Função **col()**:\n",
        "Returns a Column based on the given column name."
      ],
      "metadata": {
        "id": "OUAFY4yxPvzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nomes = nomes.withColumn(\"nome_completo\",\n",
        "                         F.concat(F.col(\"first_name\"), \n",
        "                                  F.lit(\" \"),\n",
        "                                  F.col(\"last_name\")))"
      ],
      "metadata": {
        "id": "IALLw_f_MDya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traz a nova coluna dentro de 'nomes'"
      ],
      "metadata": {
        "id": "TDL3ZR1JQKsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nomes.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BBHw99rNGwt",
        "outputId": "e4b5d64c-0055-48e2-bdce-ef83f8bdc06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+----------+---------+------------+---------------+-------------+\n",
            "|employee_id|           cpf|first_name|last_name|basic_salary|salario_dobrado|nome_completo|\n",
            "+-----------+--------------+----------+---------+------------+---------------+-------------+\n",
            "|     E-1001|557.216.676-64|    Mahesh|    Joshi|       16860|        33720.0| Mahesh Joshi|\n",
            "|     E-1002|647.770.644-09|    Rajesh|    Kolte|       14960|        29920.0| Rajesh Kolte|\n",
            "|     E-1004|785.276.601-30|     Priya|     Jain|       12670|        25340.0|   Priya Jain|\n",
            "|     E-1005|127.891.042-55|     Sneha|    Joshi|       15660|        31320.0|  Sneha Joshi|\n",
            "|     E-1007|635.807.316-88|       Ram|   Kanade|       15850|        31700.0|   Ram Kanade|\n",
            "|     E-1008|644.788.226-62|     Nishi|   Honrao|       15950|        31900.0| Nishi Honrao|\n",
            "|     E-1009|521.515.132-62|    Hameed|    Singh|       15120|        30240.0| Hameed Singh|\n",
            "+-----------+--------------+----------+---------+------------+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark vai ler novo arquivo CSV que vai ficar guardado dentro de bonus_data"
      ],
      "metadata": {
        "id": "1dN0dLbxQhLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bonus_data = spark.read.csv(\"bonus_data.csv\", header=True)"
      ],
      "metadata": {
        "id": "-DOisxg3NDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mostra o arquivo"
      ],
      "metadata": {
        "id": "vzR2Io84QrsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bonus_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZBA4V7lNrHx",
        "outputId": "862c1a21-20be-4fc8-afdd-e4b1e4c642f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|Employee_ID|Bonus|\n",
            "+-----------+-----+\n",
            "|     E-1001|16070|\n",
            "|     E-1003|15200|\n",
            "|     E-1004|13490|\n",
            "|     E-1006|14200|\n",
            "|     E-1008|15880|\n",
            "|     E-1010|15120|\n",
            "+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função **alias()**\n",
        "Returns a new DataFrame with an alias set.\n",
        "\n",
        "New in version 1.3.0.\n",
        "\n",
        "### Parameters\n",
        "alias: str\n",
        "an alias name to be set for the DataFrame.\n",
        "## Função **join()**\n",
        "Joins with another DataFrame, using the given join expression.\n",
        "\n",
        "### Parameters\n",
        "otherDataFrame\n",
        "\n",
        "Right side of the join\n",
        "\n",
        "**on**: *str, list or Column, optional*\n",
        "\n",
        "a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.\n",
        "\n",
        "**how**: *str, optional*\n",
        "\n",
        "default inner. Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti."
      ],
      "metadata": {
        "id": "JbF2uJqnQzRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nomes.alias(\"a\").join(bonus_data.alias(\"b\"),\n",
        "                      F.col(\"a.employee_id\") == F.col(\"b.Employee_ID\"), how=\"left\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FNYi0VFLlfE",
        "outputId": "02f1f19d-05e3-43fb-c222-a1b668c1b56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[employee_id: string, cpf: string, first_name: string, last_name: string, basic_salary: string, salario_dobrado: double, nome_completo: string, Employee_ID: string, Bonus: string]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função **avg()**:\n",
        "Aggregate function: returns the average of the values in a group.\n",
        "\n",
        "## Função **take()**:\n",
        "Returns the first num rows as a list of Row."
      ],
      "metadata": {
        "id": "Ha3z0fqaR59d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg = nomes.select(F.avg(\"basic_salary\")).take(1)[0][0]"
      ],
      "metadata": {
        "id": "c5vs9AMeO-Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função **when()**:\n",
        "Evaluates a list of conditions and returns one of multiple possible result expressions. If pyspark.sql.Column.otherwise() is not invoked, None is returned for unmatched conditions.\n",
        "\n",
        "New in version 1.4.0.\n",
        "\n",
        "### Parameters\n",
        "conditionColumn\n",
        "\n",
        "a boolean Column expression.\n",
        "\n",
        "value :\n",
        "a literal value, or a Column expression.\n",
        "\n",
        "## Função **otherwise()**:\n",
        "\n",
        "Evaluates a list of conditions and returns one of multiple possible result expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n",
        "\n",
        "New in version 1.4.0.\n",
        "\n",
        "### Parameters\n",
        "value\n",
        "a literal value, or a Column expression."
      ],
      "metadata": {
        "id": "pOLgh1fASzCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classificacao = nomes.withColumn(\"classificacao\",\n",
        "                 F.when(F.col(\"basic_salary\") > F.lit(avg),\n",
        "                        F.lit(\"acima_da_media\"))\\\n",
        "                 .otherwise(F.lit(\"abaixo_da_media\")))"
      ],
      "metadata": {
        "id": "YZb9B2aSS6N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificacao.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9bwUb8EUZ-E",
        "outputId": "0939fa90-f89c-4fde-e47a-087fda0802e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+----------+---------+------------+---------------+-------------+---------------+\n",
            "|employee_id|           cpf|first_name|last_name|basic_salary|salario_dobrado|nome_completo|  classificacao|\n",
            "+-----------+--------------+----------+---------+------------+---------------+-------------+---------------+\n",
            "|     E-1001|557.216.676-64|    Mahesh|    Joshi|       16860|        33720.0| Mahesh Joshi| acima_da_media|\n",
            "|     E-1002|647.770.644-09|    Rajesh|    Kolte|       14960|        29920.0| Rajesh Kolte|abaixo_da_media|\n",
            "|     E-1004|785.276.601-30|     Priya|     Jain|       12670|        25340.0|   Priya Jain|abaixo_da_media|\n",
            "|     E-1005|127.891.042-55|     Sneha|    Joshi|       15660|        31320.0|  Sneha Joshi| acima_da_media|\n",
            "|     E-1007|635.807.316-88|       Ram|   Kanade|       15850|        31700.0|   Ram Kanade| acima_da_media|\n",
            "|     E-1008|644.788.226-62|     Nishi|   Honrao|       15950|        31900.0| Nishi Honrao| acima_da_media|\n",
            "|     E-1009|521.515.132-62|    Hameed|    Singh|       15120|        30240.0| Hameed Singh|abaixo_da_media|\n",
            "+-----------+--------------+----------+---------+------------+---------------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classificacao.groupBy(\"classificacao\").sum().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydtVAJLrUMaz",
        "outputId": "50e155de-bef4-4204-9e9a-483dc148a6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+\n",
            "|  classificacao|sum(salario_dobrado)|\n",
            "+---------------+--------------------+\n",
            "|abaixo_da_media|             85500.0|\n",
            "| acima_da_media|            128640.0|\n",
            "+---------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classificacao.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt7Uj2mr3zoo",
        "outputId": "1bab9d38-e020-41e8-fc85-923a1e2d72e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Project [Employee_ID#17 AS employee_id#61, CPF#18 AS cpf#68, First_Name#19 AS first_name#75, Last_Name#20 AS last_name#82, Basic_Salary#21 AS basic_salary#89, (cast(Basic_Salary#21 as double) * 2.0) AS salario_dobrado#96, concat(First_Name#19,  , Last_Name#20) AS nome_completo#103, CASE WHEN (cast(Basic_Salary#21 as double) > 15295.714285714286) THEN acima_da_media ELSE abaixo_da_media END AS classificacao#208]\n",
            "+- FileScan csv [Employee_ID#17,CPF#18,First_Name#19,Last_Name#20,Basic_Salary#21] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/sal_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Employee_ID:string,CPF:string,First_Name:string,Last_Name:string,Basic_Salary:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_cpf(cpf: str) -> str:\n",
        "    '''Remove caracteres de numeros de CPF a fim de obfusca-los.\n",
        "    Feita para ser utilizada como uma UDF do Pyspark.'''\n",
        "\n",
        "    return \"***\" + cpf[3:9] + \"-**\" if len(cpf)==14 else len(cpf) * \"*\""
      ],
      "metadata": {
        "id": "kJ7Lrr_kU4XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_cpf(\"557.216.676-64\")"
      ],
      "metadata": {
        "id": "6DK2ZL4AT7tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as T"
      ],
      "metadata": {
        "id": "VBISB3qMAo1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mascara_cpf_udf = F.udf(mask_cpf, T.StringType())"
      ],
      "metadata": {
        "id": "Fhzp_-6HA6BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificacao.withColumn(\"cpf\", mascara_cpf_udf(F.col(\"cpf\")))\\\n",
        "  .filter(F.col(\"classificacao\") == F.lit(\"acima_da_media\"))\\\n",
        "    .drop(\"classificacao\") \\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "vQU-Fph8SB14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}